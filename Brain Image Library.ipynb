{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f41e84-896c-4a4a-bc4f-2d1fca75b8c5",
   "metadata": {},
   "source": [
    "# Brain Image Library\n",
    "\n",
    "Name:\n",
    "\n",
    "Andrew ID:\n",
    "\n",
    "* You can work in groups, it is highly encouraged. Basic divide-and-conquer.\n",
    "* But each of you will have to submit your notebook and data on Canvas.\n",
    "\n",
    "You have been given access to a database where you can pull information from one table. The purpose of this exercise is to\n",
    "\n",
    "* clean the local copy of the table\n",
    "* compute some basic information about the files in the table\n",
    "* add these features to the table\n",
    "* use the original raw data to create a second table\n",
    "* use these 2 tables to make some pretty plots\n",
    "\n",
    "In theory, the file and dataset level features you will be computing in this exercise will be stored in a database and used for a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a9189-6ea9-45b0-8277-88ca4e17ca0f",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "a. The variable `file` belows points to a CSV file with the file informations. Load the table into the workspace using Pandas or Dask.\n",
    "\n",
    "**Hint**\n",
    "If you use Pandas, set `low_memory` to `False`. For more info click [here](https://stackoverflow.com/questions/58551446/how-to-set-low-memory-to-false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f0557-ec0c-4fb1-aa19-2eb45eeb919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n",
    "file = '/bil/workshops/2022/data-science/final_project_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029df5d5-0ada-4a05-818b-6429062b68b3",
   "metadata": {},
   "source": [
    "b. Print the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73100a0-e9e2-4ed9-b5a2-b25b749a762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0409de-6eaf-4239-876c-392056e32abe",
   "metadata": {},
   "source": [
    "c. This dataframe has one line per file in the file system. The data wrangler already included some file level statistics in the table. However before we can proceed adding more columns, we need to fix some things.\n",
    "\n",
    "For example, if you take a look at the file creation dates, these look like\n",
    "\n",
    "```\n",
    "df['file_creation_date'][0]\n",
    "'2019-12-0503:26:25'\n",
    "```\n",
    "\n",
    "when in fact these should look like \n",
    "\n",
    "```\n",
    "'2019-12-05 03:26:25'\n",
    "```\n",
    "\n",
    "First change the values in the column `file_creation_date` so the dates are correct.\n",
    "\n",
    "Second, change the data type of this series to be a `datetime` object.\n",
    "\n",
    "**Hint**\n",
    "* Add a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db12f8a-c290-420d-b405-46da77dc2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f829d4a-1032-420a-8f33-a288be5d15b6",
   "metadata": {},
   "source": [
    "d. The column `file_size` has the file size in bytes. Add a column named `human_readable_file_size` that is dtype string. This string representation of the file size should be an approximation to the nearest unit with one decimal point. For example, `15M`, `1.7G` and `5.6T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b48ef-579c-4b19-8c93-283fca6eaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05f825-94c2-42ec-ad8a-506ef55739a8",
   "metadata": {},
   "source": [
    "e. The column `xxh128` is empty. To compute this hash and populate this column we will use the function below. However, due to time constraints, we will only compute these hash for files whose extensions are not `.tif`/`.tiff`.\n",
    "\n",
    "For each file in the table, whose extension is not `.tif`/`.tiff`, compute the hash and store it in the column `xxh128`. Files without this hash, should remain as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5624df5-ff17-43f9-85ab-754ba88fe591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_xxh128sum( filename ):\n",
    "    if Path(filename).is_file():\n",
    "        results = subprocess.check_output('/bil/packages/xxhash/0.8.0/xxh128sum ' + str(filename) + ' | cut -d\" \" -f1 | xargs', shell=True)\n",
    "        return results.decode(\"utf-8\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be537bf1-6da9-4699-b899-b3aa21f6606c",
   "metadata": {},
   "source": [
    "f. Add column `exists`. Use the value in `filepath` to find if the file exists on disk. If the file exists, then populate the column as `True`. `False` otherwise.\n",
    "\n",
    "**Hint** \n",
    "* Use `Path` from `pathlib`.\n",
    "* All files should exist, if some of these don't, then report it. Not your fault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeb39c-aeb8-466a-bb09-05554ef6de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54cf773-c5c6-43e0-8fff-07051f8d8b9d",
   "metadata": {},
   "source": [
    "g. Add columns `download_link`, `download_ready`, `response_code`, `download_timestamp`. Add these four columns and set the default values to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3e463-b53a-48fb-9a04-7588557c7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e87c7f-1b0e-4859-b600-b361a78aec82",
   "metadata": {},
   "source": [
    "h. We can populate the columns above at the same time to minimize the number of requests. In this case, each value in `filepath` can be turned into an download link. For example, the file `/bil/data/00/9c/009c1e6fcc03ebac/mouseID_19032506-191184/readme.txt` can be turned into a URL by replacing the prefix `/bil/data` with `https://download.brainimagelibrary.org`, leading to `https://download.brainimagelibrary.org/00/9c/009c1e6fcc03ebac/mouseID_19032506-191184/readme.txt`.\n",
    "\n",
    "Download the response header (not the file) to determine if the file is reachable. Add `True` to `download_ready` if the file is reachable. `False` otherwise.\n",
    "\n",
    "Save the response code from the request to column `response_code` as an integer, e.g. 202, 404, etc.\n",
    "\n",
    "Record the timestamp when you made this request to column `download_timestamp`. This column should be of dtype `datetime`.\n",
    "\n",
    "**Hints**\n",
    "* Write a single method that can perform these tasks in a single call.\n",
    "* You could also write a method that takes the full dataframe and returns and updated dataframe.\n",
    "* You should be saving checkpoints to avoid recomputation.\n",
    "* All links should be reachable. If some of these aren't that's a problem, but not your problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557aa499-fe40-4334-be88-b7b3d05a8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344e556-170d-4ea6-9bc1-26ec9d5235f5",
   "metadata": {},
   "source": [
    "i. Populate the column `dataset_uuid`. This one is tricky. This dataframe has one row per file. These files are grouped together in datasets. However the column `dataset_uuid` is empty. And now we need to populate it.\n",
    "\n",
    "A dataset can be identified as a combination of `collection_id`, `sample_id` and `directory`. Generate a unique [UUID](https://www.educba.com/python-uuid/) for each unique combination of `collection_id`, `sample_id` and `directory`. Use the UUID to populate the column `dataset_uuid`. Keep in mind there will be multiple rows in this table that will share the combination of `collection_id`, `sample_id` and `directory`, hence all of these rows should also have the `dataset_uuid` as it means these are all part of the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de8f43-fcc5-4d4f-af16-25c0bb9ef21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f4e6a-29fc-46e5-bcdb-ec7040e2bb22",
   "metadata": {},
   "source": [
    "k. If you haven't done it (and you should have saved some checkpoints), then save the df to disk as a pickle file and as a tsv file. Save it to `manifest.tsv` and `manifest.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57505c-c58d-43a5-b762-c6ff361838a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033c3f7-f95a-4f54-8e79-c84027b7c90b",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "a. Create another dataframe with columns `['dataset_uuid', 'collection_id', 'dataset_id', 'sample_id', 'directory', 'file_extensions','timestamp']`.\n",
    "\n",
    "i. Set `file_extensions` to `None`.\n",
    "\n",
    "ii. Use the values in the first dataframe to populate the other columns.\n",
    "\n",
    "iii. Record the timestamp when you made this request to column `timestamp`. This column should be of dtype `datetime`.\n",
    "\n",
    "**Hints**\n",
    "* The first dataframe had a row per each file in the dataset. Whereas the second dataframe should have one row per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8160077-c7cf-4a14-9022-f5d90855d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355755af-3f61-4092-86be-dc71b237f0c8",
   "metadata": {},
   "source": [
    "b. For each dataset in the first dataframe, count the number of file extensions.\n",
    "\n",
    "i. Store these values as JSON in the column `file_extensions`. For example, if a dataset has 100 `.tif`s and 1000 `.jpeg`s, then you should store the string `{'tif':100,'jpeg':1000 }`.\n",
    "\n",
    "ii. If a dataset has a file extension that is `None` or `Nan` store it as `other`. For example, `{'tif':100,'jpeg':1000, 'other':4 }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5950733-93ff-45bd-b093-8db95bc652cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b393a-f7cd-42ca-b6db-7e7c75123b66",
   "metadata": {},
   "source": [
    "c. Save the dataframe to disk as a pickle file and as a tsv file. Save it to `datasets.tsv` and `datasets.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324aa67-9b36-4a50-8eb0-fe6edbd06378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e2af4-0e5c-479f-afdb-8a7efbced120",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "a. Create another dataframe with columns `['timestamp', 'number_of_datasets', 'number_of_collections', 'number_of_files', 'file_extensions']`.\n",
    "\n",
    "i. Timestamp records the current date, not the time, e.g. 2022-01-23.\n",
    "\n",
    "ii. The number of datasets in the `datasets` dataframe goes in `number_of_datasets`.\n",
    "\n",
    "iii. The number of unique collections in the `datasets` dataframe goes in `number_of_collections`.\n",
    "\n",
    "iv. The total number of files in the `manifest` dataframe goes in `number_of_files`.\n",
    "\n",
    "v. Another tricky one. Aggregate all the file extensions in column `file_extensions` in dataframe `datasets`, collect these as a single JSON block and it to `file_extensions` in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3aa71-e8cd-4a5b-a652-511ca156b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3a707-53f7-4223-b81b-7f7eea7efc66",
   "metadata": {},
   "source": [
    "b. Save the dataframe to disk as a pickle file and as a tsv file. Save it to `info.tsv` and `info.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43b113-ee1b-4caf-9796-e93d4f49a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b4520-6ee1-4515-8598-56bf6f025642",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Create some pretty plots and embed them in this notebook. Feel free to use any plotting library in Python. Make them pretty though. \n",
    "\n",
    "a. Waffleplot. Create a waffleplot from the column `file_extensions` in the dataframe `info`. \n",
    "\n",
    "* Title should be the date in `timestamp`.\n",
    "* No axis labels.\n",
    "* Add legend.\n",
    "\n",
    "**Hints***\n",
    "* Use `pywaffle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40443223-850c-4d8f-8c39-1da4b16c0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d9451-4d90-476e-bc87-0ca1b1eb69c6",
   "metadata": {},
   "source": [
    "b. Histogram. Make a histogram using the column `download_ready` from the dataframe `manifest`. \n",
    "\n",
    "* Set title to `Broken links`\n",
    "* Values set to `False` should be labeled as `Broken`.\n",
    "* Values set to `True` should be labeled as `Not Broken`.\n",
    "* No legend.\n",
    "* Set y-label to `Number of links`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317cea2-3823-4164-a00d-f669dd575f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b3d80-f63e-451f-b0b4-f363ac066ce5",
   "metadata": {},
   "source": [
    "c. Pie chart. Create a pie chart using the values in `status_code` in the dataframe `manifest`. \n",
    "\n",
    "* Title should be `Status codes`.\n",
    "* Add legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337c178-0705-4ea1-a5c5-65a193da41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf717a-2376-49d3-b88c-e2ce53670b5d",
   "metadata": {},
   "source": [
    "d. Create a plot using the values in `file_creation_date` in dataframe manifest. However, this exercise is open-ended. \n",
    "\n",
    "This is my user-story and your job is to create the best plot you think will show what I want.\n",
    "\n",
    "* The only part I care about the `file_create_date` is the date. Or months, or years, not sure.\n",
    "* What I want to do is to create a plot that I can show the increment of data through the years.\n",
    "* I am still debating whether I should just show how many files are available per year, or do a cumulative plot since it should be, in theory, monotonically increasing.\n",
    "\n",
    "What do you think? Make a plot, convince me your plot is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d982d560-4138-4d4f-b38d-f24e80f78cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
